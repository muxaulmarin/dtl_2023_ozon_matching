{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16546729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('hackathon_files_for_participants_ozon/train_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f14aa159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>color_parsed</th>\n",
       "      <th>pic_embeddings_resnet_v1</th>\n",
       "      <th>main_pic_embeddings_resnet_v1</th>\n",
       "      <th>name_bert_64</th>\n",
       "      <th>characteristic_attributes_mapping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51195767</td>\n",
       "      <td>Удлинитель Партнер-Электро ПВС 2х0,75 ГОСТ,6A,...</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Электроника\", \"3\": \"Сетевые...</td>\n",
       "      <td>[оранжевый]</td>\n",
       "      <td>None</td>\n",
       "      <td>[[0.04603629, 0.18839523, -0.09973055, -0.6636...</td>\n",
       "      <td>[-0.47045058, 0.67237014, 0.48984158, -0.54485...</td>\n",
       "      <td>{\"Номинальный ток, А\":[\"10\"],\"Цвет товара\":[\"о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53565809</td>\n",
       "      <td>Магнитный кабель USB 2.0 A (m) - USB Type-C (m...</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Электроника\", \"3\": \"Кабели ...</td>\n",
       "      <td>[красный]</td>\n",
       "      <td>[[0.26863545, -0.3130674, 0.29023397, 0.073978...</td>\n",
       "      <td>[[1.1471839, -0.665361, 0.7745614, 0.26716197,...</td>\n",
       "      <td>[-0.6575592, 0.6522429, 0.5426037, -0.54347897...</td>\n",
       "      <td>{\"Конструктивные особенности\":[\"Магнитная конс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56763357</td>\n",
       "      <td>Набор микропрепаратов Konus 25: \"Клетки и ткан...</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Электроника\", \"3\": \"Оптичес...</td>\n",
       "      <td>None</td>\n",
       "      <td>[[0.66954195, 1.0643557, 0.78324044, -0.338267...</td>\n",
       "      <td>[[-0.90570974, 1.0296293, 1.0769907, 0.27746, ...</td>\n",
       "      <td>[-0.7384308, 0.70784587, 0.3012653, -0.3583719...</td>\n",
       "      <td>{\"Тип аксессуара\":[\"Набор микропрепаратов\"],\"Б...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56961772</td>\n",
       "      <td>Мобильный телефон BQ 1848 Step, черный</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Электроника\", \"3\": \"Смартфо...</td>\n",
       "      <td>[черный]</td>\n",
       "      <td>[[0.6580482, -0.35763323, -0.16939065, -0.4249...</td>\n",
       "      <td>[[0.13133773, -0.5577079, 0.32498044, 0.191717...</td>\n",
       "      <td>[-0.44812852, 0.5283565, 0.28981736, -0.506841...</td>\n",
       "      <td>{\"Тип карты памяти\":[\"microSD\"],\"Число SIM-кар...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61054740</td>\n",
       "      <td>Штатив трипод Tripod 330A для фотоаппаратов, в...</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Электроника\", \"3\": \"Штативы...</td>\n",
       "      <td>[черный]</td>\n",
       "      <td>[[-0.10406649, 0.080646515, -0.28668788, 0.739...</td>\n",
       "      <td>[[0.21696381, 0.10989461, -0.08012986, 0.69186...</td>\n",
       "      <td>[-0.72692573, 0.75206333, 0.37740713, -0.52502...</td>\n",
       "      <td>{\"Материал\":[\"Металл\"],\"Количество секций, шт\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variantid                                               name   \n",
       "0   51195767  Удлинитель Партнер-Электро ПВС 2х0,75 ГОСТ,6A,...  \\\n",
       "1   53565809  Магнитный кабель USB 2.0 A (m) - USB Type-C (m...   \n",
       "2   56763357  Набор микропрепаратов Konus 25: \"Клетки и ткан...   \n",
       "3   56961772             Мобильный телефон BQ 1848 Step, черный   \n",
       "4   61054740  Штатив трипод Tripod 330A для фотоаппаратов, в...   \n",
       "\n",
       "                                          categories color_parsed   \n",
       "0  {\"1\": \"EPG\", \"2\": \"Электроника\", \"3\": \"Сетевые...  [оранжевый]  \\\n",
       "1  {\"1\": \"EPG\", \"2\": \"Электроника\", \"3\": \"Кабели ...    [красный]   \n",
       "2  {\"1\": \"EPG\", \"2\": \"Электроника\", \"3\": \"Оптичес...         None   \n",
       "3  {\"1\": \"EPG\", \"2\": \"Электроника\", \"3\": \"Смартфо...     [черный]   \n",
       "4  {\"1\": \"EPG\", \"2\": \"Электроника\", \"3\": \"Штативы...     [черный]   \n",
       "\n",
       "                            pic_embeddings_resnet_v1   \n",
       "0                                               None  \\\n",
       "1  [[0.26863545, -0.3130674, 0.29023397, 0.073978...   \n",
       "2  [[0.66954195, 1.0643557, 0.78324044, -0.338267...   \n",
       "3  [[0.6580482, -0.35763323, -0.16939065, -0.4249...   \n",
       "4  [[-0.10406649, 0.080646515, -0.28668788, 0.739...   \n",
       "\n",
       "                       main_pic_embeddings_resnet_v1   \n",
       "0  [[0.04603629, 0.18839523, -0.09973055, -0.6636...  \\\n",
       "1  [[1.1471839, -0.665361, 0.7745614, 0.26716197,...   \n",
       "2  [[-0.90570974, 1.0296293, 1.0769907, 0.27746, ...   \n",
       "3  [[0.13133773, -0.5577079, 0.32498044, 0.191717...   \n",
       "4  [[0.21696381, 0.10989461, -0.08012986, 0.69186...   \n",
       "\n",
       "                                        name_bert_64   \n",
       "0  [-0.47045058, 0.67237014, 0.48984158, -0.54485...  \\\n",
       "1  [-0.6575592, 0.6522429, 0.5426037, -0.54347897...   \n",
       "2  [-0.7384308, 0.70784587, 0.3012653, -0.3583719...   \n",
       "3  [-0.44812852, 0.5283565, 0.28981736, -0.506841...   \n",
       "4  [-0.72692573, 0.75206333, 0.37740713, -0.52502...   \n",
       "\n",
       "                   characteristic_attributes_mapping  \n",
       "0  {\"Номинальный ток, А\":[\"10\"],\"Цвет товара\":[\"о...  \n",
       "1  {\"Конструктивные особенности\":[\"Магнитная конс...  \n",
       "2  {\"Тип аксессуара\":[\"Набор микропрепаратов\"],\"Б...  \n",
       "3  {\"Тип карты памяти\":[\"microSD\"],\"Число SIM-кар...  \n",
       "4  {\"Материал\":[\"Металл\"],\"Количество секций, шт\"...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b11c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pairs = pd.read_parquet('hackathon_files_for_participants_ozon/train_pairs.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77748bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cat3'] = df['categories'].apply(lambda x : eval(x)['3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9527a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Сетевые фильтры, разветвители и удлинители\n",
       "1                               Кабели и переходники\n",
       "2                                 Оптические приборы\n",
       "3            Смартфоны, планшеты, мобильные телефоны\n",
       "4                                  Штативы и головки\n",
       "                             ...                    \n",
       "457058                          Расходник для печати\n",
       "457059                      Защитные пленки и стекла\n",
       "457060                                     Компьютер\n",
       "457061                                     Компьютер\n",
       "457062       Смартфоны, планшеты, мобильные телефоны\n",
       "Name: cat3, Length: 457063, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cat3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "478b247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat3_counts = df.cat3.value_counts().to_dict()\n",
    "df[\"cat3_groupped\"] = df[\"cat3\"].apply(lambda x: x if cat3_counts[x] > 1000 else \"rest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d832ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306540/306540 [20:53<00:00, 244.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "def making_char_pairs(js1, js2):\n",
    "    res_dist, res_similar = [], []\n",
    "    try:\n",
    "        js1 = eval(js1)\n",
    "        js2 = eval(js2)\n",
    "        jskeys = set(js1.keys()) & set(js2.keys())\n",
    "    except:\n",
    "        return res_dist, res_similar\n",
    "    \n",
    "    for k in jskeys:\n",
    "        v1 = js1.get(k)\n",
    "        v2 = js2.get(k)\n",
    "        if v1 != v2:\n",
    "            res_dist.append(k)\n",
    "        if v1 == v2:\n",
    "            res_similar.append(k)\n",
    "    return res_dist, res_similar\n",
    "\n",
    "dataset = []\n",
    "for i in tqdm.tqdm(range(len(df_train_pairs))):\n",
    "    t1,t2 = df_train_pairs.iloc[i].variantid1, df_train_pairs.iloc[i].variantid2\n",
    "    target = df_train_pairs.iloc[i].target\n",
    "    category = df.loc[df.variantid == t1].cat3.values[0]\n",
    "    name1 = df.loc[df.variantid == t1].name.values[0]\n",
    "    name2 = df.loc[df.variantid == t2].name.values[0]\n",
    "    cat_groupped = df.loc[df.variantid == t1].cat3_groupped.values[0]\n",
    "    res_dist, res_similar = making_char_pairs(df.loc[df.variantid == t1].characteristic_attributes_mapping.values[0],\n",
    "                                              df.loc[df.variantid == t2].characteristic_attributes_mapping.values[0]\n",
    "                                             )\n",
    "    dataset.append((category, name1, name2, ', '.join(res_dist), ', '.join(res_similar), target, cat_groupped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2807256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "def pr_auc_macro(\n",
    "    df: pd.DataFrame,\n",
    "    prec_level: float = 0.75\n",
    ") -> float:\n",
    "        \n",
    "    y_true = df[\"target\"]\n",
    "    y_pred = df[\"scores\"]\n",
    "    categories = df[\"categories\"]\n",
    "    \n",
    "    weights = []\n",
    "    pr_aucs = []\n",
    "\n",
    "    unique_cats, counts = np.unique(categories, return_counts=True)\n",
    "\n",
    "    # calculate metric for each big category\n",
    "    for i, category in enumerate(unique_cats):\n",
    "        # take just a certain category\n",
    "        cat_idx = np.where(categories == category)[0]\n",
    "        y_pred_cat = y_pred[cat_idx]\n",
    "        y_true_cat = y_true[cat_idx]\n",
    "\n",
    "        # if there is no matches in the category then PRAUC=0\n",
    "        if sum(y_true_cat) == 0:\n",
    "            pr_aucs.append(0)\n",
    "            weights.append(counts[i] / len(categories))\n",
    "            continue\n",
    "        \n",
    "        # get coordinates (x, y) for (recall, precision) of PR-curve\n",
    "        y, x, _ = precision_recall_curve(y_true_cat, y_pred_cat)\n",
    "        \n",
    "        # reverse the lists so that x's are in ascending order (left to right)\n",
    "        y = y[::-1]\n",
    "        x = x[::-1]\n",
    "        \n",
    "        # get indices for x-coordinate (recall) where y-coordinate (precision) \n",
    "        # is higher than precision level (75% for our task)\n",
    "        good_idx = np.where(y >= prec_level)[0]\n",
    "        \n",
    "        # if there are more than one such x's (at least one is always there, \n",
    "        # it's x=0 (recall=0)) we get a grid from x=0, to the rightest x \n",
    "        # with acceptable precision\n",
    "        if len(good_idx) > 1:\n",
    "            gt_prec_level_idx = np.arange(0, good_idx[-1] + 1)\n",
    "        # if there is only one such x, then we have zeros in the top scores \n",
    "        # and the curve simply goes down sharply at x=0 and does not rise \n",
    "        # above the required precision: PRAUC=0\n",
    "        else:\n",
    "            pr_aucs.append(0)\n",
    "            weights.append(counts[i] / len(categories))\n",
    "            continue\n",
    "        \n",
    "        # calculate category weight anyway\n",
    "        weights.append(counts[i] / len(categories))\n",
    "        # calculate PRAUC for all points where the rightest x \n",
    "        # still has required precision \n",
    "        try:\n",
    "            pr_auc_prec_level = auc(x[gt_prec_level_idx], y[gt_prec_level_idx])\n",
    "            if not np.isnan(pr_auc_prec_level):\n",
    "                pr_aucs.append(pr_auc_prec_level)\n",
    "        except ValueError:\n",
    "            pr_aucs.append(0)\n",
    "            \n",
    "    return np.average(pr_aucs, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c61b02d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 0 prauc(0.75) 0.762 rocauc 0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 epoch 0 prauc(0.75) 0.767 rocauc 0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2 epoch 0 prauc(0.75) 0.764 rocauc 0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3 epoch 0 prauc(0.75) 0.762 rocauc 0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4 epoch 0 prauc(0.75) 0.763 rocauc 0.936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=239)\n",
    "ds_indexes = np.arange(len(dataset))\n",
    "\n",
    "ifold = 0\n",
    "oof = np.zeros(len(dataset))\n",
    "for tr, va in kf.split(ds_indexes):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", \n",
    "                                                               num_labels=2).cuda()\n",
    "    #model.load_state_dict(torch.load(models_by_folds[ifold]))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "    epochs = 1\n",
    "    total_steps = (1 + len(tr) // 32) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, \n",
    "                                                num_training_steps = total_steps)\n",
    "    \n",
    "    i = 0\n",
    "    train_losses = []\n",
    "    for ep in range(epochs):\n",
    "        np.random.shuffle(tr)\n",
    "        optimizer.zero_grad()\n",
    "        losses = []\n",
    "        for t in tr:\n",
    "            category, name1, name2, dist, sim, target, _ = dataset[t]\n",
    "            s = category + '[SEP]' + name1 + '[SEP]' + name2 + '[SEP]' + dist # + '[SEP]' + sim\n",
    "            tks = tokenizer.encode_plus(s[:1200], max_length=512, pad_to_max_length=False, \n",
    "                                        return_attention_mask=True, return_tensors='pt', truncation=True)\n",
    "            out = model(tks['input_ids'].cuda(), \n",
    "                        attention_mask=tks['attention_mask'].cuda(),\n",
    "                        token_type_ids=tks['token_type_ids'].cuda(),\n",
    "                        labels = torch.tensor([[1.0-target, target]]).float().cuda()\n",
    "                       )\n",
    "            #print(out.logits[0][1].item(), target)\n",
    "            losses.append(out.loss)\n",
    "            \n",
    "            i += 1\n",
    "            if i % 32 == 0:\n",
    "                loss = sum(losses) / 32.0\n",
    "                loss.backward()\n",
    "                losses = []\n",
    "                train_losses.append(loss.item())\n",
    "                optimizer.step() \n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                \n",
    "        if len(losses) > 0:\n",
    "            loss = sum(losses) / 32.0\n",
    "            loss.backward()\n",
    "            losses = []\n",
    "            train_losses.append(loss.item())\n",
    "            optimizer.step() \n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            \n",
    "        evaldf = []\n",
    "        for t in va:\n",
    "            category, name1, name2, dist, sim, target, cat_groupped = dataset[t]\n",
    "            s = category + '[SEP]' + name1 + '[SEP]' + name2 + '[SEP]'  + dist #+ '[SEP]' + sim\n",
    "            tks = tokenizer.encode_plus(s[:1200], max_length=512, pad_to_max_length=False,\n",
    "                                return_attention_mask=True, return_tensors='pt', truncation=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                score = model(tks['input_ids'].cuda(), \n",
    "                              attention_mask=tks['attention_mask'].cuda(),\n",
    "                              token_type_ids=tks['token_type_ids'].cuda()).logits[0][1].item()\n",
    "            evaldf.append((target, score, cat_groupped))\n",
    "        evaldf = pd.DataFrame(evaldf)\n",
    "        evaldf.columns = [\"target\", \"scores\", \"categories\"]\n",
    "        m = pr_auc_macro(evaldf)\n",
    "        m2 = roc_auc_score(evaldf.target.values, evaldf.scores.values)\n",
    "        print('fold', ifold, 'epoch', ep, 'prauc(0.75)', round(m,3), 'rocauc', round(m2,3))\n",
    "        torch.save(model.state_dict(), f'chstic_{ifold}_{round(m,3)}_{round(m2,3)}.pth')\n",
    "        oof[va] = evaldf.scores.values\n",
    "    ifold += 1\n",
    "    \n",
    "df_oof = pd.read_parquet('hackathon_files_for_participants_ozon/train_pairs.parquet')\n",
    "df_oof['chstic'] = oof\n",
    "df_oof.to_parquet('oof_chstic.parquet')\n",
    "\n",
    "nn_features = pd.read_parquet('oof_mbert.parquet')[\"variantid1\",\"variantid2\",\"mbert\"]\n",
    "nn_features = nn_features.merge(evaldf, on=[\"variantid1\",\"variantid2\"], how='left')\n",
    "nn_features.to_parquet('nn/train.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
